# 模型调整与预训练

这段时间的学习下来主要发现了几个问题。首先是数据的问题，数据在归一化的过程中，在进行Resize和CenterCrop的时候容易出现，缩放，裁剪不明确的问题。归一化过程中的均值和标准差调整问题等。

这个总结主要是集中一些问题点，和预训练模型进行的

# 1.Finetune模型微调
```
# 冻结模型参数的函数
def set_parameter_requires_grad(model, feature_extracting):
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False
            
# 冻结参数的梯度
feature_extract = True
# pretrained=True 为使用原有的模型参数进行初始化训练，为False的话就只使用模型结构。
model = models.resnet18(pretrained=True)
set_parameter_requires_grad(model, feature_extract)

# 修改模型
num_ftrs = model.fc.in_features
model.fc = nn.Linear(in_features=num_ftrs, out_features=128, bias=True)
model.conv3 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3,stride=2)
model.bn3 = nn.BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
model.relu
model.conv4 = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3,stride=2)
```
修改模型是可以修改已有的resnet18当中的模型结构和模型参数的，可以直接使用model.进行修改。

# 2.画LOSS曲线图
最简单的一种方法
```
plt.plot(loss_list)
plt.legend()
plt.title('Compare loss for different models in training')
```
将LOSS和ACC画到一起的
```
from mpl_toolkits.axes_grid1 import host_subplot
def plot_acc_loss(loss, acc):
    host = host_subplot(111)  # row=1 col=1 first pic
    plt.subplots_adjust(right=0.8)  # ajust the right boundary of the plot window
    par1 = host.twinx()   # 共享x轴
 
    # set labels
    host.set_xlabel("steps")
    host.set_ylabel("train-loss")
    par1.set_ylabel("train-accuracy")
 
    # plot curves
    p1, = host.plot(range(len(loss)), loss, label="loss")
    #p2, = par1.plot(range(len(acc)), acc, label="accuracy")
 
    # set location of the legend,
    # 1->rightup corner, 2->leftup corner, 3->leftdown corner
    # 4->rightdown corner, 5->rightmid ...
    host.legend(loc=5)
 
    # set label color
    host.axis["left"].label.set_color(p1.get_color())
    par1.axis["right"].label.set_color(p2.get_color())
 
    # set the range of x axis of host and y axis of par1
    # host.set_xlim([-200, 5200])
    # par1.set_ylim([-0.1, 1.1])
 
    plt.draw()
    plt.show()

plot_acc_loss(loss_list, acc_list)
```
# 3.计算准确度
```
@torch.no_grad()
def get_all_preds(model, loader):
    all_preds = torch.tensor([])
    for batch in loader:
        images, labels = batch

        preds = model(images)
        all_preds = torch.cat((all_preds, preds) ,dim=0)

    return all_preds

# 计算训练集的准确度
train_preds = get_all_preds(network, train_loader)
actual_labels_train = torch.Tensor(train_loader.labels)
preds_correct_train = train_preds.argmax(dim=1).eq(actual_labels_train).sum().item()

print('accuracy_train:', preds_correct_train / len(actual_labels_train))

# 计算测试集的准确度

test_preds = get_all_preds(network, test_loader)
actual_labels_test = torch.Tensor(test_data.labels)
preds_correct_test = test_preds.argmax(dim=1).eq(actual_labels_test).sum().item()

print('total correct:', preds_correct_test)
print('accuracy_test:', preds_correct_test / len(test_data))
```
在这里要注意，如果是batch_size不能整除len(train_data)的话，也就是说有一部分数据没有被用到，那么就会导致在计算train_preds.argmax(dim=1).eq(actual_labels_train).sum().item()的时候出现数据量不同的问题。这个问题暂时没有比较好的解决方案，推荐在数据集划分的时候，尽量将数据集划分成能被batch_size整除的。
所以在分割数据集的时候我使用了这样的划分：
```
import torch
from torch.utils.data import random_split
batch_size =4
dataset = list(label)
train_dataset, test_dataset = random_split(
    dataset=dataset,
    lengths=[int(len(label)*0.8)//batch_size*batch_size, (len(label)-int(0.8*len(label))//batch_size*batch_size)],
    generator=torch.Generator().manual_seed(0)
)
```
最近的两篇文章都不是系统性的总结，先总结下来之后进行修改吧。
